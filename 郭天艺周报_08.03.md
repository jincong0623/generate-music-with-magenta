#郭天艺周报——第二周

### 任务：

1. 学习python的基本语句（1）
2. 机器学习初步

### 完成情况：

1. python基础：莫烦python教学 1—10
2. 莫烦播单：有趣的机器学习 1—8

### 笔记：

#### 1. 机器学习分类：
- **监督学习**（supervised learning）：有数据和标签的
- **非监督学习**（unsupervised learning）：只有数据 无标签
- **半监督学习**：结合监督学习与非监督学习
- **强化学习**（reinforcement learning）：从经验中总结提升的
- **遗传算法**（与强化学习相似）（genetic algorithm）：适者生存 不适者淘汰
2. **本质**：++误差反向传递++。给予信息以及由信息经神经系统得出的“反应”，
给系统以多个给定的信息以及结果的数据，让神经系统不断反馈错误的经验，
++进化正确的链路，淘汰不正确的链路++，最后形成制定反应的数学模型。

3. **神经网络**：input layer——hidden layer——output layer；
训练过程：输入——识别——数据分析处理——判断——输出——判断输出结果与给定结果的差异——若正确：对隐藏层的激活过的函数优化、不激活未激活的函数，若错误：激活未激活的函数、不激活激活过的函数。
4. **神经网络中的隐藏层**：
- 提取输入层的信息，加工转化成多个特征信息，并作为下一个隐藏层的输入信息
- 以此类推，++层层进行特征分析，输入为上一层的分析结果输出++
- 得到的所有特征信息在坐标中降落在不同的区域，得出特征信息对应的输入
数据所在的位置可得出结果分类
- 迁移学习：使用迁移的方式将已知的神经网络作为输入层进行另一种目的的训练学习

5. **梯度下降机制**（Gradient Descent in NN）：由误差方程得出误差最小的地方（梯度躺平点）

6. **卷积神经网络**：（对图片）过滤器对每个小区域进行拾取提取边缘信息而并非像素点

7. **循环神经网络RNN**（Recurrent Neural Networks）：
- 普通神经网络是单个NN对于单个信息一对一进行处理得出单个结果，且之间没有顺序关联
- 而RNN利用RAM储存之前的所有分析数据并调用：++输入X（t）输出Y（t），每次分析后RNN生成的状态描述被记录为S（t）++

8. **长短期记忆LSTM**（Long Short-Term Memory）：
- RNN存在梯度消失效应（对于有用的信息但是是靠前出现的，最后得出的结果的误差经过反向传递，w<1时，累乘因数w，误差趋于0）；梯度爆炸效应（w>1时，累乘因数w，误差趋于无穷大）
- LSTM—RNN加入了++输入控制、输出控制、忘记控制++
- LSTM存在一条控制全局的记忆——主线，每层RNN——分线（含有输入控制、输出控制、忘记控制）
- 输入：把对主线重要的数据写入再进行分析；忘记：层中数据产生改变时，对数据进行比较并对新旧数据进行取舍；输出：基于主线以及分线数据判断输出数据