**目标**：

零基础入门深度学习(1) - 感知器

零基础入门深度学习(2) - 线性单元和梯度下降

零基础入门深度学习(3) - 神经网络和反向传播算法（部分）

地址：

1：https://www.zybuluo.com/hanbingtao/note/433855

2：https://www.zybuluo.com/hanbingtao/note/448086

3：https://www.zybuluo.com/hanbingtao/note/476663#an1



**感知器部分**：

![2256672-c6f640c11a06ac2e](D:\智能编曲项目\论文\周报\2017.7.27\2256672-c6f640c11a06ac2e.png)

神经网络如上图所示，每个圆圈代表着一个神经元，而线则代表着神经元之间的联系，层与层之间的神经元有着密切联系，而层内的神经元却无联系。最左边的这层叫做输入层，中间的这层叫做隐藏层，最右边的这层叫做输出层。

隐藏层大于2层的神经网络叫做深度神经网络。在处理同一个函数时，它会比浅层网络使用更少的神经元，节约更多的资源，但是却不好训练。



**感知器**

神经元也叫做感知器。

下图为一个感知器

![感知器](D:\智能编曲项目\论文\周报\2017.7.27\感知器.png)

一个感知器分为三个部分

1：输入权重：每个输入Xi都有一个输入权重Wi，并且还有一个偏置项，即上图的W0。

2：激活函数：可以有很多种选择，比如阶跃函数f。

3：输出：由下列公式来计算：y=f(w*x+b).

感知器可以拟合所有的线性函数，任何的现行分类和线性回归都可以用感知器来处理。

感知器所拟合的线性函数都与输入权重和偏置项有关，所以感知器的训练就是寻找一个合适的正确的输入权重和偏置项。

可以利用感知器规则迭代来修改Wi和b：

![8708ae7acc8322a54b163b451f68035](C:\Users\张章\AppData\Local\Temp\WeChat Files\8708ae7acc8322a54b163b451f68035.png)

其中：

![e2dd77223eae31f2b1fa6cef9e6d7f4](C:\Users\张章\AppData\Local\Temp\WeChat Files\e2dd77223eae31f2b1fa6cef9e6d7f4.png)

t是训练时的样本值，一般称之为label，y是感知器的输出值，而（t-y）的系数则是一个称为学习速率的常数，其作用是控制每一步调整权的幅度。没处理一个样本就调整一次权重，经过多轮迭代后，就可以训练出正确的感知器的权重。



线性单元和梯度下降：

![9f9c41160a39dfba3ad4468a13f613b](C:\Users\张章\AppData\Local\Temp\WeChat Files\9f9c41160a39dfba3ad4468a13f613b.png)

线性单元模型![803a086f4eae475446ef9f1a25cd93d](C:\Users\张章\AppData\Local\Temp\WeChat Files\803a086f4eae475446ef9f1a25cd93d.png)



**监督学习和无监督学习****：

监督学习：每个训练样本既包括输入特征x，也包括对应的输出y（也叫做标记）。

无监督学习：训练样本中只有x没有y。

梯度下降算法：![img](file:///C:/Users/%E5%BC%A0%E7%AB%A0/AppData/Local/Temp/WeChat%20Files/9f9c41160a39dfba3ad4468a13f613b.png?lastModify=1500991273)

以多轮迭代的方式找到函数的最小值

目标函数的梯度：![a965e55669f2b868af1c30d99945d1a](C:\Users\张章\AppData\Local\Temp\WeChat Files\a965e55669f2b868af1c30d99945d1a.png)

梯度上升算法：![626209f58f89e115666b3a5bddb677c](C:\Users\张章\AppData\Local\Temp\WeChat Files\626209f58f89e115666b3a5bddb677c.png)

以多轮迭代的方式找到函数的最大值。

**随机梯度下降算法（SGD）**

​      只计算一个样本，就可以完成w的迭代的更新。这样对于一个具有百万样本的训练数据，完成一遍历练就会对w更新数百万次；

一个机器的学习算法分为两部分：

1.模型：从输入特征x来预测输出y的的个函数h（x）；

2.目标函数：目标函数取最小（最大）值所对应的参数值，就是模型参数的最优值；



**神经元**

神经元本质上和感知器是一样的，只是激活函数会不一样，神经元的俄机或函数一般是sigmoid函数或者是tanh函数。

**反向传播算法**

先假设每个训练样本为（***x***，***t***），其中向量***x***是训练的样本的特征，而***t***是样本的目标值

![2256672-6f27ced45cf5c0d8](C:\Users\张章\Pictures\2256672-6f27ced45cf5c0d8.png)

先求出每个节点的误差项

- 对输出节点：

  ![8bfaaa4a0ccec727076b01e9605deee](C:\Users\张章\AppData\Local\Temp\WeChat Files\8bfaaa4a0ccec727076b01e9605deee.png)

  yi是节点的输出项，t1是样本对应与节点i的目标值

- 对于隐藏层节点：![da8c560aa456c6251d04982ad64c7bf](C:\Users\张章\AppData\Local\Temp\WeChat Files\da8c560aa456c6251d04982ad64c7bf.png)

ai是节点i的输出值，Wki是节点i到他下一节点k的连接权重。

最后更新每个连接上的权值：![799bbcd02633aa008a7ec59bb311156](C:\Users\张章\AppData\Local\Temp\WeChat Files\799bbcd02633aa008a7ec59bb311156.png)











