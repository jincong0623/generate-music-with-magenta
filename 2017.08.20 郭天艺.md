### import模块
```
improt time
import time as t
from time import time,localtime 
//只导入两个功能--直接调用，无需用time.localtime()调用
from time import*
//导入time中的全部功能
```

### import自己的模块
```

//建立m1.py文件脚本
def printdata(data) print('data')
//m1.py 中
import m1
m1.printdata('I am python1')
//模块文件放在同一目录下
//或者放在site-packages中
```
### 跳出循环语句
```
# continue  跳过一次循环
# break     跳出整个循环
```
### 错误处理try
```
try:
    file = open('eeee','r+')// 不存在的文件
except Exception as e:
    print(e)//将错误信息存在e变量中
    print('there is no file named as eeee ')
    response = input('do you want to create a new file')
    if response == 'y':
        file = open('eeee','w')
    else:
        pass
else: //若文件可以打开做的执行
    file.write('ssss')
file.close()
```
### zip lambda map
```
//zip
a = [1,2,3]
b = [4,5,6]
zip(a,b)
list(zip(a,b))
//结果为[(1,4),(2,5),(3,6)]

for i,j in zip(a,b):
    print(i,j)
//输出(a,b)

list(zip(a,a,b))
//可迭代更多

//lambda
fun = lambda x,y:x+y//更简单地定义函数
fun(2,3) //输出5

//map
list(map(fun,[1，2],[3，4]))//输出[4,6]
//更简便地调用函数
```

### copy deepcopy
```
import copy
a = [1,2,3]
b = a
id(a)//a在硬盘中所占用的索引
//id(a)与id(b)相同，即引用
c = copy.copy(a)
//copy使用了浅复制即只复制了a的值

a = [1,2,[3,4]]
d = copy.copy(a)
id(a) == id(d)//False
id(a[2]) == id(d[2])//True 同为[3,4]
//a,d的id不同 不会随对方改变
//a[2],d[2]的id相同 [3,4]会随对方改变

e = copy.deepcopy(a)
id(e[2]) == id(a[2])//False
//deepcopy所有的内容都是数据传递 没有地址重复
```

### tensorflow
```
/////////////////////////////训练程序///////////////////////////
import tensorflow as tf
import numpy as np

x_data = np.random.rand(100).astype(np.float32)
y_data = x_data*0.1+0.3

Weights = tf.Variable(tf.random_uniform([1],-1.0,1.0))
biases = tf.Variable(tf.zeros([1]))

y = Weights*x_data + biases

loss = tf.reduce_mean(tf.square(y-y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
#优化器
train = optimizer.minimize(loss)
#优化器减少误差

init = tf.initialize_all_variables()


sess = tf.Session()
sess.run(init)
#激活 IMPORTANT！

for step in range(201):
    sess.run(train)
    if step % 20 == 0:      #每20步打印一次
        print(step,sess.run(Weights),sess.run(biases))
        
        
/////////////////////////////训练结果///////////////////////////
0 [ 0.08131221] [ 0.44779485]
20 [ 0.08010532] [ 0.31126365]
40 [ 0.094894] [ 0.30289084]
60 [ 0.09868955] [ 0.30074194]
80 [ 0.09966367] [ 0.30019042]
100 [ 0.09991368] [ 0.30004889]
120 [ 0.09997783] [ 0.30001256]
140 [ 0.0999943] [ 0.30000323]
160 [ 0.09999853] [ 0.30000085]
180 [ 0.09999963] [ 0.30000022]
200 [ 0.09999991] [ 0.30000007]

```
### Session 会话控制
```
/////////////////////////////程序///////////////////////////
import tensorflow as tf

matrix1 = tf.constant([[3,3]])#一行两列
matrix2 = tf.constant([[2],
                        [2]])#两行一列
product = tf.matmul(matrix1,matrix2) #matrix multiply np.dot(m1,m2)

#两种Session的打开方式

#method 1
sess = tf.Session() #Session是object要大写首字母
result = sess.run(product)
print(result)
sess.close()

#method 2
with tf.Session() as sess:#包括了打开-关闭的步骤
    result2 = sess.run(product)
    print(result2)

/////////////////////////////结果///////////////////////////
[[12]]
[[12]]
```

### Variable 变量
```
/////////////////////////////程序///////////////////////////
import tensorflow as tf


state = tf.Variable(0,name='counter')#定义变量初值和名字
print(state.name)
one = tf.constant(1)#为1的常量

new_value = tf.add(state,one)#new_value仅为变量而不是Variable变量
update = tf.assign(state,new_value)#将new_value的值加载到state上

init = tf.initialize_all_variables()#初始化所有变量，只要有变量必须要有这一步

with tf.Session() as sess:
    sess.run(init)#每一个变量时初始化所有变量
    for _ in range(3):
        sess.run(update)
        print(sess.run(state))
        
/////////////////////////////结果///////////////////////////
counter:0
1
2
3
```
### Placeholder 传入值
```
/////////////////////////////程序///////////////////////////
import tensorflow as tf

input1 = tf.placeholder(tf.float32)#可以规定两行两列(tf.float32,[2,2])
input2 = tf.placeholder(tf.float32)

output = tf.add(input1,input2)

with tf.Session() as sess:
    print(sess.run(output,feed_dict={input1:[7.],input2:[2.]}))#后通过feed_dict传入值
/////////////////////////////结果///////////////////////////
[ 9.]
```
### 激励函数
y(预测值) = AF(Wx)（输入值）
卷积神经网络：relu
循环神经网络：relu or tanh


# Midinet（1）
### 摘要
- 基于GAN的一种CNN——Midinet，旨在提供一般的、用于符号音乐的高度自适应网络结构。
- 适用于1D或2D的条件。1D：运用预计的和弦和现有的小节A；2D：之前的小节A和由A生成的小节B。
- 以随机的噪声为输入，一小节一小节地生成音符序列；每次的输出为16*128的矩阵，表示每小节的旋律的记录基础为以16分音符为节拍单位，128个MIDI音符的旋律序列。多个矩阵串联即为音乐序列的输出结果。

### 介绍
- 基于生成式对抗网络GAN
- 基于卷积神经网络CNN
- 用于生成主旋律序列，而不是生成不断的连续旋律序列
- 通过1D条件可以添加更多相关组成的高度自适应结构，用于复杂的、分层时态模式（跨时间）、调和关系（主旋律和伴随音）。
- 通过2D的随时间推移音符作为CNN的输入。与时间相关旋律之间的依赖可以不使用RNN中的重复单元进行建模，而在音乐中建立长期的时间依赖性。
